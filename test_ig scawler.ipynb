{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "defaults = {\n",
    "    \"fetch_likes_plays\": False,\n",
    "    \"fetch_likers\": False,\n",
    "    \"fetch_comments\": False,\n",
    "    \"fetch_mentions\": False,\n",
    "    \"fetch_hashtags\": False,\n",
    "    \"fetch_details\": False\n",
    "}\n",
    "\n",
    "\n",
    "def apply_defaults(cls):\n",
    "    for name, value in defaults.items():\n",
    "        setattr(cls, name, value)\n",
    "    return cls\n",
    "\n",
    "\n",
    "@apply_defaults\n",
    "class settings(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "def override_settings(args):\n",
    "    for name in defaults.keys():\n",
    "        setattr(settings, name, getattr(args, name))\n",
    "\n",
    "\n",
    "def prepare_override_settings(parser):\n",
    "    for name in defaults.keys():\n",
    "        parser.add_argument(\"--\" + name, action=\"store_true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .crawler import InsCrawler\n",
    "\n",
    "# browser.py\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from .utils import randmized_sleep\n",
    "\n",
    "\n",
    "class Browser:\n",
    "    def __init__(self, has_screen):\n",
    "        dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        service_args = [\"--ignore-ssl-errors=true\"]\n",
    "        chrome_options = Options()\n",
    "        if not has_screen:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--start-maximized\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        self.driver = webdriver.Chrome(\n",
    "            executable_path=\"%s/bin/chromedriver\" % dir_path,\n",
    "            service_args=service_args,\n",
    "            chrome_options=chrome_options,\n",
    "        )\n",
    "        self.driver.implicitly_wait(5)\n",
    "\n",
    "    @property\n",
    "    def page_height(self):\n",
    "        return self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    def get(self, url):\n",
    "        self.driver.get(url)\n",
    "\n",
    "    @property\n",
    "    def current_url(self):\n",
    "        return self.driver.current_url\n",
    "\n",
    "    def implicitly_wait(self, t):\n",
    "        self.driver.implicitly_wait(t)\n",
    "\n",
    "    def find_one(self, css_selector, elem=None, waittime=0):\n",
    "        obj = elem or self.driver\n",
    "\n",
    "        if waittime:\n",
    "            WebDriverWait(obj, waittime).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, css_selector))\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            return obj.find_element(By.CSS_SELECTOR, css_selector)\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "    def find(self, css_selector, elem=None, waittime=0):\n",
    "        obj = elem or self.driver\n",
    "\n",
    "        try:\n",
    "            if waittime:\n",
    "                WebDriverWait(obj, waittime).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, css_selector))\n",
    "                )\n",
    "        except TimeoutException:\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            return obj.find_elements(By.CSS_SELECTOR, css_selector)\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "    def scroll_down(self, wait=0.3):\n",
    "        self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        randmized_sleep(wait)\n",
    "\n",
    "    def scroll_up(self, offset=-1, wait=2):\n",
    "        if offset == -1:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, 0)\")\n",
    "        else:\n",
    "            self.driver.execute_script(\"window.scrollBy(0, -%s)\" % offset)\n",
    "        randmized_sleep(wait)\n",
    "\n",
    "    def js_click(self, elem):\n",
    "        self.driver.execute_script(\"arguments[0].click();\", elem)\n",
    "\n",
    "    def open_new_tab(self, url):\n",
    "        self.driver.execute_script(\"window.open('%s');\" %url)\n",
    "        self.driver.switch_to.window(self.driver.window_handles[1])\n",
    "\n",
    "    def close_current_tab(self):\n",
    "        self.driver.close()\n",
    "\n",
    "        self.driver.switch_to.window(self.driver.window_handles[0])\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.driver.quit()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler.py\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "from inscrawler import InsCrawler\n",
    "from inscrawler.settings import override_settings\n",
    "from inscrawler.settings import prepare_override_settings\n",
    "\n",
    "\n",
    "def usage():\n",
    "    return \"\"\"\n",
    "        python crawler.py posts -u cal_foodie -n 100 -o ./output\n",
    "        python crawler.py posts_full -u cal_foodie -n 100 -o ./output\n",
    "        python crawler.py profile -u cal_foodie -o ./output\n",
    "        python crawler.py profile_script -u cal_foodie -o ./output\n",
    "        python crawler.py hashtag -t taiwan -o ./output\n",
    "\n",
    "        The default number for fetching posts via hashtag is 100.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_posts_by_user(username, number, detail, debug):\n",
    "    ins_crawler = InsCrawler(has_screen=debug)\n",
    "    return ins_crawler.get_user_posts(username, number, detail)\n",
    "\n",
    "\n",
    "def get_profile(username):\n",
    "    ins_crawler = InsCrawler()\n",
    "    return ins_crawler.get_user_profile(username)\n",
    "\n",
    "\n",
    "def get_profile_from_script(username):\n",
    "    ins_cralwer = InsCrawler()\n",
    "    return ins_cralwer.get_user_profile_from_script_shared_data(username)\n",
    "\n",
    "\n",
    "def get_posts_by_hashtag(tag, number, debug):\n",
    "    ins_crawler = InsCrawler(has_screen=debug)\n",
    "    return ins_crawler.get_latest_posts_by_tag(tag, number)\n",
    "\n",
    "\n",
    "def arg_required(args, fields=[]):\n",
    "    for field in fields:\n",
    "        if not getattr(args, field):\n",
    "            parser.print_help()\n",
    "            sys.exit()\n",
    "\n",
    "\n",
    "def output(data, filepath):\n",
    "    out = json.dumps(data, ensure_ascii=False)\n",
    "    if filepath:\n",
    "        with open(filepath, \"w\", encoding=\"utf8\") as f:\n",
    "            f.write(out)\n",
    "    else:\n",
    "        print(out)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Instagram Crawler\", usage=usage())\n",
    "    parser.add_argument(\n",
    "        \"mode\", help=\"options: [posts, posts_full, profile, profile_script, hashtag]\"\n",
    "    )\n",
    "    parser.add_argument(\"-n\", \"--number\", type=int, help=\"number of returned posts\")\n",
    "    parser.add_argument(\"-u\", \"--username\", help=\"instagram's username\")\n",
    "    parser.add_argument(\"-t\", \"--tag\", help=\"instagram's tag name\")\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"output file name(json format)\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "\n",
    "    prepare_override_settings(parser)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    override_settings(args)\n",
    "\n",
    "    if args.mode in [\"posts\", \"posts_full\"]:\n",
    "        arg_required(\"username\")\n",
    "        output(\n",
    "            get_posts_by_user(\n",
    "                args.username, args.number, args.mode == \"posts_full\", args.debug\n",
    "            ),\n",
    "            args.output,\n",
    "        )\n",
    "    elif args.mode == \"profile\":\n",
    "        arg_required(\"username\")\n",
    "        output(get_profile(args.username), args.output)\n",
    "    elif args.mode == \"profile_script\":\n",
    "        arg_required(\"username\")\n",
    "        output(get_profile_from_script(args.username), args.output)\n",
    "    elif args.mode == \"hashtag\":\n",
    "        arg_required(\"tag\")\n",
    "        output(\n",
    "            get_posts_by_hashtag(args.tag, args.number or 100, args.debug), args.output\n",
    "        )\n",
    "    else:\n",
    "        usage()\n",
    "        \n",
    "class RetryException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch.py\n",
    "\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "from .settings import settings\n",
    "\n",
    "\n",
    "def get_parsed_mentions(raw_text):\n",
    "    regex = re.compile(r\"@([\\w\\.]+)\")\n",
    "    regex.findall(raw_text)\n",
    "    return regex.findall(raw_text)\n",
    "\n",
    "\n",
    "def get_parsed_hashtags(raw_text):\n",
    "    regex = re.compile(r\"#(\\w+)\")\n",
    "    regex.findall(raw_text)\n",
    "    return regex.findall(raw_text)\n",
    "\n",
    "\n",
    "def fetch_mentions(raw_test, dict_obj):\n",
    "    if not settings.fetch_mentions:\n",
    "        return\n",
    "\n",
    "    mentions = get_parsed_mentions(raw_test)\n",
    "    if mentions:\n",
    "        dict_obj[\"mentions\"] = mentions\n",
    "\n",
    "def fetch_hashtags(raw_test, dict_obj):\n",
    "    if not settings.fetch_hashtags:\n",
    "        return\n",
    "\n",
    "    hashtags = get_parsed_hashtags(raw_test)\n",
    "    if hashtags:\n",
    "        dict_obj[\"hashtags\"] = hashtags\n",
    "\n",
    "\n",
    "def fetch_datetime(browser, dict_post):\n",
    "    ele_datetime = browser.find_one(\".eo2As .c-Yi7 ._1o9PC\")\n",
    "    datetime = ele_datetime.get_attribute(\"datetime\")\n",
    "    dict_post[\"datetime\"] = datetime\n",
    "\n",
    "\n",
    "def fetch_imgs(browser, dict_post):\n",
    "    img_urls = set()\n",
    "    while True:\n",
    "        ele_imgs = browser.find(\"._97aPb img\", waittime=10)\n",
    "\n",
    "        if isinstance(ele_imgs, list):\n",
    "            for ele_img in ele_imgs:\n",
    "                img_urls.add(ele_img.get_attribute(\"src\"))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        next_photo_btn = browser.find_one(\"._6CZji .coreSpriteRightChevron\")\n",
    "\n",
    "        if next_photo_btn:\n",
    "            next_photo_btn.click()\n",
    "            sleep(0.3)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    dict_post[\"img_urls\"] = list(img_urls)\n",
    "\n",
    "def fetch_likes_plays(browser, dict_post):\n",
    "    if not settings.fetch_likes_plays:\n",
    "        return\n",
    "\n",
    "    likes = None\n",
    "    el_likes = browser.find_one(\".Nm9Fw > * > span\")\n",
    "    el_see_likes = browser.find_one(\".vcOH2\")\n",
    "\n",
    "    if el_see_likes is not None:\n",
    "        el_plays = browser.find_one(\".vcOH2 > span\")\n",
    "        dict_post[\"views\"] = int(el_plays.text.replace(\",\", \"\").replace(\".\", \"\"))\n",
    "        el_see_likes.click()\n",
    "        el_likes = browser.find_one(\".vJRqr > span\")\n",
    "        likes = el_likes.text\n",
    "        browser.find_one(\".QhbhU\").click()\n",
    "\n",
    "    elif el_likes is not None:\n",
    "        likes = el_likes.text\n",
    "\n",
    "    dict_post[\"likes\"] = (\n",
    "        int(likes.replace(\",\", \"\").replace(\".\", \"\")) if likes is not None else 0\n",
    "    )\n",
    "\n",
    "\n",
    "def fetch_likers(browser, dict_post):\n",
    "    if not settings.fetch_likers:\n",
    "        return\n",
    "    like_info_btn = browser.find_one(\".EDfFK ._0mzm-.sqdOP\")\n",
    "    like_info_btn.click()\n",
    "\n",
    "    likers = {}\n",
    "    liker_elems_css_selector = \".Igw0E ._7UhW9.xLCgt a\"\n",
    "    likers_elems = list(browser.find(liker_elems_css_selector))\n",
    "    last_liker = None\n",
    "    while likers_elems:\n",
    "        for ele in likers_elems:\n",
    "            likers[ele.get_attribute(\"href\")] = ele.get_attribute(\"title\")\n",
    "\n",
    "        if last_liker == likers_elems[-1]:\n",
    "            break\n",
    "\n",
    "        last_liker = likers_elems[-1]\n",
    "        last_liker.location_once_scrolled_into_view\n",
    "        sleep(0.6)\n",
    "        likers_elems = list(browser.find(liker_elems_css_selector))\n",
    "\n",
    "    dict_post[\"likers\"] = list(likers.values())\n",
    "    close_btn = browser.find_one(\".WaOAr button\")\n",
    "    close_btn.click()\n",
    "\n",
    "\n",
    "def fetch_caption(browser, dict_post):\n",
    "    ele_comments = browser.find(\".eo2As .gElp9\")\n",
    "\n",
    "    if len(ele_comments) > 0:\n",
    "\n",
    "        temp_element = browser.find(\"span\",ele_comments[0])\n",
    "\n",
    "        for element in temp_element:\n",
    "\n",
    "            if element.text not in ['Verified',''] and 'caption' not in dict_post:\n",
    "                dict_post[\"caption\"] = element.text\n",
    "\n",
    "        fetch_mentions(dict_post.get(\"caption\",\"\"), dict_post)\n",
    "        fetch_hashtags(dict_post.get(\"caption\",\"\"), dict_post)\n",
    "\n",
    "\n",
    "def fetch_comments(browser, dict_post):\n",
    "    if not settings.fetch_comments:\n",
    "        return\n",
    "\n",
    "    show_more_selector = \"button .glyphsSpriteCircle_add__outline__24__grey_9\"\n",
    "    show_more = browser.find_one(show_more_selector)\n",
    "    while show_more:\n",
    "        show_more.location_once_scrolled_into_view\n",
    "        show_more.click()\n",
    "        sleep(0.3)\n",
    "        show_more = browser.find_one(show_more_selector)\n",
    "\n",
    "    show_comment_btns = browser.find(\".EizgU\")\n",
    "    for show_comment_btn in show_comment_btns:\n",
    "        show_comment_btn.location_once_scrolled_into_view\n",
    "        show_comment_btn.click()\n",
    "        sleep(0.3)\n",
    "\n",
    "    ele_comments = browser.find(\".eo2As .gElp9\")\n",
    "    comments = []\n",
    "    for els_comment in ele_comments[1:]:\n",
    "        author = browser.find_one(\".FPmhX\", els_comment).text\n",
    "\n",
    "        temp_element = browser.find(\"span\", els_comment)\n",
    "\n",
    "        for element in temp_element:\n",
    "\n",
    "            if element.text not in ['Verified','']:\n",
    "                comment = element.text\n",
    "\n",
    "        comment_obj = {\"author\": author, \"comment\": comment}\n",
    "\n",
    "        fetch_mentions(comment, comment_obj)\n",
    "        fetch_hashtags(comment, comment_obj)\n",
    "\n",
    "        comments.append(comment_obj)\n",
    "\n",
    "    if comments:\n",
    "        dict_post[\"comments\"] = comments\n",
    "\n",
    "\n",
    "def fetch_initial_comment(browser, dict_post):\n",
    "    comments_elem = browser.find_one(\"ul.XQXOT\")\n",
    "    first_post_elem = browser.find_one(\".ZyFrc\", comments_elem)\n",
    "    caption = browser.find_one(\"span\", first_post_elem)\n",
    "\n",
    "    if caption:\n",
    "        dict_post[\"description\"] = caption.text\n",
    "\n",
    "\n",
    "def fetch_details(browser, dict_post):\n",
    "    if not settings.fetch_details:\n",
    "        return\n",
    "\n",
    "    browser.open_new_tab(dict_post[\"key\"])\n",
    "\n",
    "    username = browser.find_one(\"a.ZIAjV\")\n",
    "    location = browser.find_one(\"a.O4GlU\")\n",
    "\n",
    "    if username:\n",
    "        dict_post[\"username\"] = username.text\n",
    "    if location:\n",
    "        dict_post[\"location\"] = location.text\n",
    "\n",
    "    fetch_initial_comment(browser, dict_post)\n",
    "\n",
    "    browser.close_current_tab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liker.py\n",
    "\n",
    "import argparse\n",
    "\n",
    "from inscrawler import InsCrawler\n",
    "\n",
    "\n",
    "def usage():\n",
    "    return \"\"\"\n",
    "        python crawler.py [tag]\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Instagram Liker\", usage=usage())\n",
    "    parser.add_argument(\"hashtag\", help=\"hashtag name\")\n",
    "    parser.add_argument(\n",
    "        \"-n\", \"--number\", type=int, default=1000, help=\"number of posts to like\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    ins_crawler = InsCrawler(has_screen=True)\n",
    "    ins_crawler.auto_like(tag=args.hashtag, maximum=args.number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import random\n",
    "from functools import wraps\n",
    "from time import sleep\n",
    "\n",
    "from .exceptions import RetryException\n",
    "\n",
    "\n",
    "def instagram_int(string):\n",
    "    return int(string.replace(\",\", \"\"))\n",
    "\n",
    "\n",
    "def retry(attempt=10, wait=0.3):\n",
    "    def wrap(func):\n",
    "        @wraps(func)\n",
    "        def wrapped_f(*args, **kwargs):\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except RetryException:\n",
    "                if attempt > 1:\n",
    "                    sleep(wait)\n",
    "                    return retry(attempt - 1, wait)(func)(*args, **kwargs)\n",
    "                else:\n",
    "                    exc = RetryException()\n",
    "                    exc.__cause__ = None\n",
    "                    raise exc\n",
    "\n",
    "        return wrapped_f\n",
    "\n",
    "    return wrap\n",
    "\n",
    "\n",
    "def randmized_sleep(average=1):\n",
    "    _min, _max = average * 1 / 2, average * 3 / 2\n",
    "    sleep(random.uniform(_min, _max))\n",
    "\n",
    "\n",
    "def validate_posts(dict_posts):\n",
    "    \"\"\"\n",
    "        The validator is to verify if the posts are fetched wrong.\n",
    "        Ex. the content got messed up or duplicated.\n",
    "    \"\"\"\n",
    "    posts = dict_posts.values()\n",
    "    contents = [post[\"datetime\"] for post in posts]\n",
    "    # assert len(set(contents)) == len(contents)\n",
    "    if len(set(contents)) == len(contents):\n",
    "        print(\"These post data should be correct.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
